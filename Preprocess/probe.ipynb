{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Set pairs\n",
    "\n",
    "# Init param\n",
    "cause_conn = []  # Set cause_conn\n",
    "\n",
    "# Load connectives info\n",
    "with open ('../data/cause_conn_modified.txt', 'r', encoding='utf-8') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        for word in line.split(','):\n",
    "            cause_conn.append(word)\n",
    "        line = f.readline()\n",
    "\n",
    "# Init csv for recording result\n",
    "with open ('../data/pairs.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "            csv_writer = csv.writer(f)\n",
    "            csv_writer.writerow(['pair_type', 'section', 'clause_index', 'candidate_index', 'clause', 'candidate', 'correctness'])\n",
    "\n",
    "# Init csv for recording raw\n",
    "with open ('../data/discourse.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "            csv_writer = csv.writer(f)\n",
    "            csv_writer.writerow(['section', 'discourse', 'word_count', 'doc_len', 'clause_len', 'ec_emotion_pos', 'ec_cause_pos', 'ce_cause_pos', 'ce_emotion_pos', 'ec_true_pairs', 'ce_true_pairs'])\n",
    "\n",
    "with open ('../data/all_data_pair.txt', 'r', encoding='utf-8') as f:  # Encode by utf-8 for Chinese\n",
    "    sec = f.readline()  # Read section ID and length\n",
    "\n",
    "    # For each section\n",
    "    while sec:\n",
    "\n",
    "        # Get section length\n",
    "        num = sec.split(' ')\n",
    "        section = int(num[0])\n",
    "        # if section > 200:\n",
    "        #     break\n",
    "        length = int(num[1])\n",
    "        content = ['' for i in range(length)]\n",
    "        refined_content = ['' for i in range(length)]\n",
    "\n",
    "        pairs = f.readline().lstrip().rstrip()  # Get the index of pairs and delete the beginning ' ' and ending '\\n'\n",
    "\n",
    "        # Get the index of pairs (int)\n",
    "        pairs_index = []\n",
    "        for pair in pairs.split(', '):\n",
    "            pairs_index.append(list(map(int, pair.lstrip('(').rstrip(')').split(','))))\n",
    "\n",
    "        # Get the content of section\n",
    "        sum_len = 0\n",
    "        word_count = 0\n",
    "        sentence_len = []\n",
    "        for i in range(length):\n",
    "            content[i] = f.readline().lstrip().rstrip().split(',')[3]\n",
    "\n",
    "            # Get the raw content\n",
    "            for word in content[i].split(' '):\n",
    "                refined_content[i] += word\n",
    "        \n",
    "            sum_len += 2 + len(refined_content[i])\n",
    "            word_count += len(refined_content[i])\n",
    "            sentence_len.append(len(refined_content[i]))\n",
    "        \n",
    "        # Set Bert_trunk (pass)\n",
    "        if sum_len > 512:\n",
    "            sec = f.readline()\n",
    "            continue\n",
    "\n",
    "        # Get emo_index and cau_index\n",
    "        ec_emo_index = []\n",
    "        ec_cau_index = []\n",
    "        ce_emo_index = []\n",
    "        ce_cau_index = []\n",
    "        for pair in pairs_index:\n",
    "            if pair[0] not in ec_emo_index:\n",
    "                ec_emo_index.append(pair[0])  # pair[0] = emo_index\n",
    "                ec_cau_index.append([])\n",
    "            ec_cau_index[ec_emo_index.index(pair[0])].append(pair[1])  # pair[1] = cau_index\n",
    "        for pair in pairs_index:\n",
    "            if pair[1] not in ce_cau_index:\n",
    "                ce_cau_index.append(pair[1])  # pair[1] = cau_index\n",
    "                ce_emo_index.append([])\n",
    "            ce_emo_index[ce_cau_index.index(pair[1])].append(pair[0])  # pair[0] = emo_index\n",
    "\n",
    "        merged_content = ''\n",
    "        for item in refined_content:\n",
    "            merged_content += item\n",
    "        \n",
    "        ec_pairs_index = pairs_index\n",
    "        ce_pairs_index = [[pair_index[1], pair_index[0]] for pair_index in pairs_index]\n",
    "        \n",
    "        with open ('../data/discourse.csv', 'a', encoding='utf-8', newline='') as g:\n",
    "            csv_writer = csv.writer(g)\n",
    "            csv_writer.writerow([section, merged_content, word_count, length, sentence_len, ec_emo_index, ec_cau_index, ce_cau_index, ce_emo_index, ec_pairs_index, ce_pairs_index])\n",
    "\n",
    "        # Delete original connectives\n",
    "        for i in range(length):\n",
    "\n",
    "            # Delete bigram first\n",
    "            del_pos = []\n",
    "            new_content = ''\n",
    "            for j in range(len(refined_content[i]) - 1):\n",
    "                if refined_content[i][j:j+2] in cause_conn:\n",
    "                    del_pos.extend([j,j+1])\n",
    "            for j in range(len(refined_content[i])):\n",
    "                if j not in del_pos:\n",
    "                    new_content = new_content + refined_content[i][j]\n",
    "            refined_content[i] = new_content\n",
    "\n",
    "            # Delete unigram later\n",
    "            del_pos = []\n",
    "            new_content = ''\n",
    "            for j in range(len(refined_content[i])):\n",
    "                if refined_content[i][j] in cause_conn:\n",
    "                    del_pos.append(j)\n",
    "            for j in range(len(refined_content[i])):\n",
    "                if j not in del_pos:\n",
    "                    new_content = new_content + refined_content[i][j]\n",
    "            refined_content[i] = new_content\n",
    "\n",
    "            # Padding\n",
    "            if refined_content[i] == '':\n",
    "                refined_content[i] = '[UNK]'\n",
    "\n",
    "        '''\n",
    "        # For each emo_clause\n",
    "        # Construct pairs\n",
    "        correctness = ''\n",
    "        for emo_clause_index in emo_index:\n",
    "            for i in range(length):\n",
    "                if i + 1 not in emo_index:\n",
    "                    cau_candidate_index = i + 1\n",
    "                    # pairs = '[CLS]' + refined_content[emo_clause_index - 1] + '[SEP]' + '[MASK]' + refined_content[i] + '[SEP]'\n",
    "                    emotion_clause = refined_content[emo_clause_index - 1]\n",
    "                    cause_candidate = refined_content[i]\n",
    "                    correctness = 'false'\n",
    "                    if i + 1 in cau_index[emo_index.index(emo_clause_index)]:\n",
    "                        correctness = 'true'\n",
    "                    \n",
    "                    # Write result in csv\n",
    "                    with open ('data/test/pairs.csv', 'a', encoding='utf-8', newline='') as g:\n",
    "                        csv_writer = csv.writer(g)\n",
    "                        csv_writer.writerow([section, emo_clause_index, cau_candidate_index, emotion_clause, cause_candidate, correctness])\n",
    "\n",
    "            # For each emo_clause considering itself\n",
    "            cau_candidate_index = emo_clause_index\n",
    "            emotion_clause = refined_content[emo_clause_index - 1]\n",
    "            cause_candidate = refined_content[cau_candidate_index - 1]\n",
    "            correctness = 'false'\n",
    "            if cau_candidate_index in cau_index[emo_index.index(emo_clause_index)]:\n",
    "                correctness = 'true'\n",
    "            \n",
    "            # Write result in csv\n",
    "            with open ('data/test/pairs.csv', 'a', encoding='utf-8', newline='') as g:\n",
    "                csv_writer = csv.writer(g)\n",
    "                csv_writer.writerow([section, emo_clause_index, cau_candidate_index, emotion_clause, cause_candidate, correctness])\n",
    "        '''\n",
    "        \n",
    "        # For each emo_clause\n",
    "        # Construct paris\n",
    "        correctness = ''\n",
    "        pair_type = 'ec'\n",
    "        for emo_clause_index in ec_emo_index:\n",
    "            for i in range(length):\n",
    "                cau_candidate_index = i + 1\n",
    "                # pairs = '[CLS]' + refined_content[emo_clause_index - 1] + '[SEP]' + '[MASK]' + refined_content[i] + '[SEP]'\n",
    "                emotion_clause = refined_content[emo_clause_index - 1]\n",
    "                cause_candidate = refined_content[i]\n",
    "                correctness = 'false'\n",
    "                if i + 1 in ec_cau_index[ec_emo_index.index(emo_clause_index)]:\n",
    "                    correctness = 'true'\n",
    "                    \n",
    "                # Write result in csv\n",
    "                with open ('../data/pairs.csv', 'a', encoding='utf-8', newline='') as g:\n",
    "                    csv_writer = csv.writer(g)\n",
    "                    csv_writer.writerow([pair_type, section, emo_clause_index, cau_candidate_index, emotion_clause, cause_candidate, correctness])\n",
    "                    \n",
    "        # For each cau_clause\n",
    "        # Construct paris\n",
    "        correctness = ''\n",
    "        pair_type = 'ce'\n",
    "        for cau_clause_index in ce_cau_index:\n",
    "            for i in range(length):\n",
    "                emo_candidate_index = i + 1\n",
    "                # pairs = '[CLS]' + refined_content[cau_clause_index - 1] + '[SEP]' + '[MASK]' + refined_content[i] + '[SEP]'\n",
    "                cause_clause = refined_content[cau_clause_index - 1]\n",
    "                emotion_candidate = refined_content[i]\n",
    "                correctness = 'false'\n",
    "                if i + 1 in ce_emo_index[ce_cau_index.index(cau_clause_index)]:\n",
    "                    correctness = 'true'\n",
    "                    \n",
    "                # Write result in csv\n",
    "                with open ('../data/pairs.csv', 'a', encoding='utf-8', newline='') as g:\n",
    "                    csv_writer = csv.writer(g)\n",
    "                    csv_writer.writerow([pair_type, section, cau_clause_index, emo_candidate_index, cause_clause, emotion_candidate, correctness])\n",
    "        \n",
    "        sec = f.readline()  # Read following section length"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
